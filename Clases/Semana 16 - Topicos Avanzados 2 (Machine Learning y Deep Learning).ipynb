{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Este notebook fue creado por Francisca Cattan (2024) para el workshop de final de curso IIC2233 Proramación Avanzada.\n",
        "\n",
        "fpcattan@uc.cl"
      ],
      "metadata": {
        "id": "_OpDgpG1DmDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning\n",
        "<img src='https://miro.medium.com/v2/resize:fit:640/format:webp/0*zRc3gOev20t1przG.jpg'>\n",
        "\n",
        "El Machine Learning es una rama de la inteligencia artificial que permite a las máquinas aprender a partir de datos, sin necesidad de ser programadas explícitamente para cada tarea. En lugar de definir reglas manuales para resolver un problema, los modelos de ML aprenden patrones y relaciones directamente desde los datos.\n",
        "\n",
        "Los modelos de ML aprenden directamente de datos estructurados como tablas numéricas. Más datos generalmente permiten construir modelos más precisos. Un buen balance entre la dificultad de la tarea, la disponibilidad y cantidad de datos, y la complejidad del modelo es fundamental.\n",
        "\n",
        "Se usa en una variedad de problemas, como:\n",
        "- Clasificación (spam/no spam, música).\n",
        "- Regresión (predicción de precios, costos, popularidad).\n",
        "- Clustering (agrupación de perfiles de grupos, estilos).\n",
        "\n",
        "### Pasos de ML (e IA en general)\n",
        "1. **Datos**: El proceso comienza con datos tabulares o simples que deben estar bien estructurados.\n",
        "2. **Preprocesamiento**: Incluye tareas como limpieza de datos, escalado y manejo de valores faltantes para preparar los datos para el modelo.\n",
        "3. **Ingeniería de Características**: Se diseñan manualmente variables relevantes (features) basadas en el conocimiento del dominio y el problema.\n",
        "4. **Modelo**: Se selecciona y entrena un modelo como SVM, Regresión Logística, o Árboles de Decisión.\n",
        "Predicción:\n",
        "El modelo predice el resultado en base a las características procesadas."
      ],
      "metadata": {
        "id": "LgsGMwLnAbuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preparacion de datos: Dataset Iris\n",
        "Trabajaremos con el set de datos **Iris** de **[Seaborn](https://seaborn.pydata.org/index.html)**. Seaborn es una librería de visualización, mucho mas potente en variedad que `matplotlib`. Además, cuenta con datos dentro de la librería. Esto es muy usual en las librerías actuales de ML y DL, los cuales tienen ya empaquetados no sólo modelos si no que también datasets. Veamos los datos disponibles en Seaborn."
      ],
      "metadata": {
        "id": "xq65h40y-TlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.get_dataset_names()"
      ],
      "metadata": {
        "id": "vaRo-9kLBifL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El dataset **Iris** tiene 50 muestras de cada una de las tres especies de flores de Iris (Setosa, Virginica y Versicolor). Se midieron cuatro características (en centímetros) de cada muestra: Largo y Ancho de los Sépalos y Pétalos.\n",
        "\n",
        "Tratemos de tener una vista resumida de este conjunto de datos:"
      ],
      "metadata": {
        "id": "qrWej4UFBnc-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zTIRA5MU89cT"
      },
      "outputs": [],
      "source": [
        "# importamos las primeras librerias\n",
        "\"\"\"\n",
        "%matplotlib inline magic command específico de Jupyter Notebook. Asegura que los\n",
        "gráficos generados con matplotlib se muestren directamente en la celda de salida\n",
        "del notebook, en lugar de abrirse en una ventana separada.\n",
        "\"\"\"\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# el dataset viene dentro de la libreria Seaborn\n",
        "iris = sns.load_dataset('iris')\n",
        "iris.head(10)"
      ],
      "metadata": {
        "id": "SwQor8dB_JLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colab tiene una herramienta de integración con los Dataframes de Pandas, que permite replicar los datos en un documento tabulado de Google (Google Sheets)."
      ],
      "metadata": {
        "id": "X-J7MulO_gjv"
      }
    },
    {
      "source": [
        "from google.colab import sheets\n",
        "sheet = sheets.InteractiveSheet(df=iris)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "id": "sCL4x0Bp_WA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(iris, hue='species', size=1.5)"
      ],
      "metadata": {
        "id": "u2koqnWB_sBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a seleccionar el X, que son los datos de entrenamiento\n",
        "X = iris.drop('species',axis=1) # sacamos las especies, que son las etiquetas\n",
        "print(X.shape)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "cxOlTfS0_1fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creamos las etiquetas, ya que queremos clasificar nuevas flores.\n",
        "y = iris['species']\n",
        "print(y.shape)\n",
        "y.describe()"
      ],
      "metadata": {
        "id": "pI-mhG4EANmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Sets de entrenamiento y test\n",
        "\n",
        "[Scikit-learn](https://scikit-learn.org/1.5/index.html) (o sklearn) es una biblioteca de aprendizaje automático en Python que proporciona herramientas simples, eficientes y fáciles de usar para realizar tareas de análisis predictivo y procesamiento de datos.\n",
        "\n",
        "- Modelado supervisado: Clasificación y regresión (e.g., SVM, Árboles de decisión, Redes neuronales).\n",
        "- Modelado no supervisado: Clustering, reducción de dimensionalidad (e.g., K-Means, PCA).\n",
        "- Preprocesamiento de datos: Normalización, escalado y manejo de datos faltantes.\n",
        "- Evaluación de modelos: Validación cruzada, métricas de evaluación.\n",
        "\n",
        "Ahora vamos a usar la librería `sklearn` para importar metricas y dividir los datos. `sklearn` nos entrega facilmente una forma de dividir. Revisar la documentacion del modulo [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn.model_selection.train_test_split)."
      ],
      "metadata": {
        "id": "UUe-pr49DD0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "ef0MyAqsDAhF"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertimos las etiquetas (y) de texto a valores numéricos usando LabelEncoder\n",
        "# Esto es necesario porque XGBoost solo acepta etiquetas numéricas.\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "print(\"Etiquetas transformadas:\")\n",
        "# mapeo entre las etiquetas originales y los números asignados.\n",
        "print(label_encoder.classes_)  # ['setosa' 'versicolor' 'virginica']"
      ],
      "metadata": {
        "id": "LaDsFxaxJp4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividimos los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "jdcxMSzdKcod"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Entrenamiento\n",
        "\n",
        "Existen muchas técnicas de entrenamiento de Machine Learning. A continuación detallaré las más clásicas, y las más usadas en la actualidad.\n",
        "\n",
        "### Técnicas clásicas\n",
        "- **Regresión Lineal y Logística**: Modelos simples para problemas de regresión (continuos) y clasificación binaria o multiclase.\n",
        "  - Uso clásico: Excelentes para datos pequeños y bien estructurados donde la interpretabilidad es importante.\n",
        "  - Por qué aún se usan: Son rápidos, interpretables y funcionan como líneas base sólidas.\n",
        "  - Por qué a veces no se usan: No manejan bien relaciones no lineales complejas o datos de alta dimensionalidad.\n",
        "\n",
        "- **Árboles de Decisión**: Dividen los datos en regiones homogéneas usando reglas basadas en características.\n",
        "  - Uso clásico: Simples de entender e interpretar; funcionan bien en datos estructurados.\n",
        "  - Por qué aún se usan: Interpretación directa y rápida.\n",
        "  - Por qué a veces no se usan: Pueden sobreajustarse fácilmente; los modelos ensamblados (Random Forest, Gradient Boosting) los han reemplazado en muchos casos.\n",
        "\n",
        "- **K-Nearest Neighbors** (KNN): Clasifica puntos basándose en las clases más comunes entre sus vecinos más cercanos.\n",
        "  - Uso clásico: Simple y no requiere entrenamiento explícito.\n",
        "  - Por qué aún se usan: Útil para prototipos rápidos y problemas simples.\n",
        "  - Por qué a veces no se usan: Ineficientes en conjuntos de datos grandes y propensos al ruido.\n",
        "\n",
        "### Técnicas usadas en la actualidad\n",
        "- **Gradient Boosting Machines** (XGBoost, LightGBM, CatBoost): Ensamblan árboles de decisión de manera secuencial para minimizar errores.\n",
        "  - Uso contemporáneo: Domina en competiciones como Kaggle debido a su capacidad de manejar datos estructurados.\n",
        "  - Por qué se usan más: Son rápidos, manejan características faltantes y logran excelente rendimiento en problemas tabulares.\n",
        "  - Por qué desplazaron técnicas clásicas: Superan fácilmente a árboles individuales y SVM en muchos casos.\n",
        "\n",
        "- **Modelos de Ensamblaje** (Stacking, Bagging, Boosting): Combina múltiples modelos para mejorar la precisión general.\n",
        "  - Uso contemporáneo: Populares por mejorar la robustez y el rendimiento general de los modelos.\n",
        "  - Por qué se usan más: Aprovechan la diversidad de modelos para mitigar errores individuales.\n",
        "  - Por qué desplazaron técnicas clásicas: Ofrecen mejores resultados que un solo modelo en problemas complejos.\n",
        "\n",
        "- **Métodos de Visualización y Reducción de Dimensionalidad** (UMAP, t-SNE): Transforman datos complejos en dimensiones más bajas para visualización y análisis.\n",
        "  - Uso contemporáneo: Cruciales para interpretar relaciones complejas en datos de alta dimensionalidad.\n",
        "  - Por qué se usan más: Mejor visualización e interpretación en comparación con métodos como PCA.\n",
        "  - Por qué desplazaron métodos clásicos: Capturan relaciones no lineales mejor que PCA.\n",
        "\n",
        "- **Redes Neuronales** (Deep Learning): Modelos altamente flexibles que pueden aprender relaciones complejas en los datos.\n",
        "  - Uso contemporáneo: Fundamental en tareas con grandes volúmenes de datos (imágenes, texto, audio).\n",
        "  - Por qué se usan más: Su capacidad para aprender patrones no lineales las hace indispensables para datos complejos y no estructurados.\n",
        "  - Por qué desplazaron técnicas clásicas: Superan a modelos tradicionales en escalabilidad y precisión en tareas específicas, **aunque requieren más recursos**."
      ],
      "metadata": {
        "id": "P15r_IaPEbwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a utilizar XGBoost, para que tengan desde ya un conocimiento de una de las herramientas más avanzadas. Probablemente el modelo es mucho mejor que"
      ],
      "metadata": {
        "id": "eN0ONPcQIQUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Ust7ilFdJ3eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones en el conjunto de prueba\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "cp9VZVpYEHpv"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación del modelo\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred, target_names=list(map(str, label_encoder.classes_)))) # vuelve a convertirlo en texto"
      ],
      "metadata": {
        "id": "YZ42IKGyK1m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f' Matriz de confusión: \\n {metrics.confusion_matrix(y_test, y_pred)}')\n",
        "# Aqui usamos la libreria metrics\n",
        "print(f' Precisión del modelo:  {metrics.balanced_accuracy_score(y_test, y_pred)}')"
      ],
      "metadata": {
        "id": "ToO_kJlcLHWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Interpretación del resultado**\n",
        "- **El modelo tuvo un desempeño perfecto** en el conjunto de prueba. Clasificó correctamente todos los ejemplos de las tres clases (`setosa`, `versicolor`, `virginica`).\n",
        "- **Desempeño \"perfecto\"** como este puede indicar que el problema es sencillo para el modelo, o que el conjunto de prueba es pequeño y está bien representado por los datos de entrenamiento."
      ],
      "metadata": {
        "id": "68MAid9iNgWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Explicación de métricas\n",
        "\n",
        "\n",
        "### Explicación del Reporte de Clasificación\n",
        "\n",
        "Los resultados del **reporte de clasificación** muestran una evaluación detallada del desempeño de un modelo de clasificación. Aquí les dejo el detalle de los términos clave y lo que significan en este caso:\n",
        "\n",
        "### **Secciones del reporte de clasificación**\n",
        "\n",
        "1. **Clases (0, 1, 2)**:\n",
        "   - Estas son las etiquetas de las clases, que representan las tres especies de iris:\n",
        "     - `0`: setosa\n",
        "     - `1`: versicolor\n",
        "     - `2`: virginica\n",
        "\n",
        "2. **Precision (Precisión)**:\n",
        "   - Mide qué proporción de las predicciones para una clase específica son correctas.\n",
        "   - **Fórmula**:  \n",
        "     ```\n",
        "     Precisión = TP / (TP + FP)\n",
        "     ```\n",
        "   - En este caso, el modelo tiene una precisión de **1.00** (100%) para todas las clases, lo que significa que no cometió errores al identificar ejemplos de cada clase.\n",
        "\n",
        "3. **Recall (Exhaustividad)**:\n",
        "   - Mide qué proporción de los casos reales de una clase fueron correctamente identificados por el modelo.\n",
        "   - **Fórmula**:  \n",
        "     ```\n",
        "     Recall = TP / (TP + FN)\n",
        "     ```\n",
        "   - El modelo tiene un recall de **1.00** (100%) para todas las clases, lo que significa que identificó correctamente todos los ejemplos reales de cada clase.\n",
        "\n",
        "4. **F1-Score**:\n",
        "   - Es la media armónica de la precisión y el recall, un valor único que equilibra ambas métricas.\n",
        "   - **Fórmula**:  \n",
        "     ```\n",
        "     F1 = 2 * (Precisión * Recall) / (Precisión + Recall)\n",
        "     ```\n",
        "   - En este caso, el F1-Score es **1.00** para todas las clases, indicando un balance perfecto entre precisión y recall.\n",
        "\n",
        "5. **Support (Soporte)**:\n",
        "   - Es la cantidad de ejemplos reales de cada clase en el conjunto de prueba. Balancear correctamente el dataset puede ser el éxito o el talón de aquiles de tu tarea.\n",
        "   - En este caso:\n",
        "     - Clase `0` (setosa): 10 ejemplos.\n",
        "     - Clase `1` (versicolor): 9 ejemplos.\n",
        "     - Clase `2` (virginica): 11 ejemplos.\n",
        "\n",
        "\n",
        "### **Métricas generales**\n",
        "\n",
        "1. **Accuracy (Precisión general)**:\n",
        "   - Es el porcentaje de predicciones correctas sobre el total de ejemplos.\n",
        "   - **Fórmula**:  \n",
        "     ```\n",
        "     Accuracy = Predicciones correctas / Total de ejemplos\n",
        "     ```\n",
        "   - En este caso, es **1.00** (100%), lo que significa que el modelo clasificó correctamente todos los ejemplos del conjunto de prueba.\n",
        "\n",
        "2. **Macro avg**:\n",
        "   - Promedio aritmético de las métricas (precisión, recall, F1) calculado por igual para cada clase, sin importar el número de ejemplos.\n",
        "   - En este caso, cada clase tiene las mismas métricas, por lo que el macro promedio es **1.00**.\n",
        "\n",
        "3. **Weighted avg**:\n",
        "   - Promedio ponderado de las métricas (precisión, recall, F1), considerando el número de ejemplos en cada clase.\n",
        "   - También es **1.00** aquí porque el modelo tuvo un desempeño perfecto.\n",
        "\n",
        "4. **Matriz de confusión**:\n",
        "  - La matriz es un resumen de las predicciones del modelo para cada clase. Cada fila representa las instancias reales de una clase, mientras que cada columna representa las instancias predichas como esa clase.\n",
        "\n",
        "---\n",
        "\n",
        "### **Nota importante**\n",
        "Aunque este resultado es excelente, en un caso real, deberías:\n",
        "- Evaluar el modelo en un conjunto de validación externo.\n",
        "- Verificar que no haya **sobreajuste** (overfitting) al entrenar el modelo. Esto se podría hacer utilizando validación cruzada.\n"
      ],
      "metadata": {
        "id": "iLeFPLSrMGs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning\n",
        "\n",
        "<img src='https://i.redd.it/zjxo20plpms81.jpg'>\n",
        "\n",
        "¿Qué es el Deep Learning (DL)?\n",
        "Deep Learning es una rama del Machine Learning que utiliza redes neuronales profundas (deep neural networks) para aprender representaciones complejas de datos. Estas redes están formadas por múltiples capas, que permiten capturar patrones y relaciones de alto nivel en los datos. Eso si, el agente MAS imoortante del avance hacia el DL es la disponibilidad de grandes cantidades de datos (Big Data) y avances en hardware, como las **GPUs**, lo que permitió el entrenamiento continuo de redes profundas que antes eran imprácticas o inviables por tiempo.\n",
        "\n",
        "**Machine Learning Clásico:**\n",
        "- Se basa en datos tabulares o variables sencillas.\n",
        "- Requiere ingeniería de características manual para crear representaciones útiles para el modelo.\n",
        "- Es ideal para problemas estructurados con datos bien definidos.\n",
        "\n",
        "**Deep Learning:**\n",
        "- Trabaja directamente con datos complejos como **imágenes, texto, audio, video, modelos 3d**. Es decir, puedes darle la entrada \"cruda\" directamente al modelo.\n",
        "- Utiliza redes neuronales profundas que aprenden automáticamente representaciones jerárquicas de los datos.\n",
        "- Es capaz de manejar problemas no lineales y de alta dimensionalidad.\n",
        "\n",
        "### Los datos que necesitamos hoy en día son complejos\n",
        "Una imagen de 256x256 píxeles tiene 65,536 características por canal. Una señal de audio de 1 minuto a 16 kHz tiene 960,000 puntos de datos. En imágenes, por ejemplo, el mismo objeto puede variar por rotación, escala o iluminación.\n",
        "En texto, una palabra puede tener significados diferentes según el contexto.\n"
      ],
      "metadata": {
        "id": "RswI43iU-VZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ¿Cómo se ve el código de una red profunda?\n",
        "\n",
        "Esta clase llamada Net es una **red neuronal convolucional (CNN)** y es capaz de procesar información sobre los pixeles y los canales de la imagen. Contiene dos capas convolucionales y luego dos capas lineales, encargadas de hacer la clasificación.\n",
        "\n",
        "### **1. Arquitectura General**\n",
        "- **Tipo de red**: CNN\n",
        "- **Entrada**: Imágenes con 3 canales (RGB).\n",
        "- **Salida**: Vector de tamaño 10, donde cada valor representa la probabilidad de que la imagen pertenezca a una de las 10 clases.\n",
        "\n",
        "### **2. Componentes de la Red**\n",
        "1. **Capa convolucional 1 (`conv1`)**:\n",
        "   - Aplica 6 filtros (kernels) de tamaño \\(5 \\times 5\\) a las imágenes de entrada.\n",
        "   - Extrae características básicas como bordes o texturas.\n",
        "\n",
        "2. **Capa de pooling (`pool`)**:\n",
        "   - Reduce las dimensiones espaciales de los mapas de características (altura y ancho) usando **MaxPooling** con tamaño \\(2 \\times 2\\) y stride 2.\n",
        "   - Esto hace que la red sea más eficiente y robusta frente a pequeñas variaciones en las imágenes.\n",
        "\n",
        "3. **Capa convolucional 2 (`conv2`)**:\n",
        "   - Aplica 16 filtros de tamaño \\(5 \\times 5\\) a los mapas de características generados por la primera convolución.\n",
        "   - Extrae características más complejas.\n",
        "\n",
        "4. **Capa lineal 1 (`fc1`)**:\n",
        "   - Aplana los mapas de características generados por la última capa convolucional en un vector de tamaño \\(16 \\times 5 \\times 5\\).\n",
        "   - Transforma este vector en un espacio de características de tamaño 120.\n",
        "\n",
        "5. **Capa lineal 2 (`fc2`)**:\n",
        "   - Reduce las 120 características a un vector de tamaño 10, correspondiente a las 10 clases a clasificar.\n",
        "\n",
        "\n",
        "### **3. Flujo de Datos (Forward)**\n",
        "1. **Entrada**:\n",
        "   - Una imagen RGB de tamaño \\(32 \\times 32\\) (como las de CIFAR-10).\n",
        "2. **Paso 1**:\n",
        "   - La primera capa convolucional transforma la entrada en un mapa de características con 6 canales.\n",
        "   - Aplicación de activación **ReLU** para introducir no linealidad.\n",
        "   - Reducción de tamaño con **MaxPooling**.\n",
        "3. **Paso 2**:\n",
        "   - La segunda capa convolucional toma los 6 mapas de características, genera 16 nuevos mapas, y aplica pooling nuevamente.\n",
        "4. **Paso 3**:\n",
        "   - El tensor de salida (dimensiones: \\(16 \\times 5 \\times 5\\)) se aplana en un vector.\n",
        "   - Este vector pasa por la primera capa lineal (`fc1`) para transformarlo a 120 características.\n",
        "5. **Paso 4**:\n",
        "   - La segunda capa lineal (`fc2`) reduce las 120 características a 10 valores.\n",
        "   - Cada valor representa la probabilidad de una clase después de aplicar **softmax** (si se utiliza).\n",
        "\n",
        "### **4. Conceptos Clave**\n",
        "1. **Convolución**:\n",
        "   - Detecta patrones locales en las imágenes, como bordes o texturas.\n",
        "2. **Pooling**:\n",
        "   - Reduce el tamaño de los mapas de características para hacer la red más eficiente y menos sensible a pequeñas variaciones.\n",
        "3. **Capas lineales (fully connected)**:\n",
        "   - Combinan características aprendidas en las capas anteriores para tomar decisiones finales."
      ],
      "metadata": {
        "id": "1P6GHjU3ep9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # primera capa convolucional\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        # un proceso de max pooling entre capas\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # una segunda capa convolucional\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # dos capas lineales\n",
        "        # la primer transforma la salida de la conv a un vector de 120\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        # la segunda transforma a un vector de 10, una capa para cada label a clasificar\n",
        "        self.fc2 = nn.Linear(120, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # View es similar a flatten, se realiza un reshape de la forma (batch_size, dimension del vector)\n",
        "        # 16 son la cantidad de canales del la capa convolucional de la capa anterior\n",
        "        # 5 * 5 es el valor del alto por ancho del mapa de activación\n",
        "        # Con este view, el vector x queda con dimension (batch_size, 16*5*5)\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net().to(device)\n"
      ],
      "metadata": {
        "id": "Mqia57Ezf4TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Plataformas integradas de desarrollo en DL\n",
        "\n",
        "Hoy en día sólo los investigadores trabajan directamente con las capas de una red profunda, sus sistemas de optimización, y todos los detalles que se estudian en postgrados o instituciones como OpenAI o DeepMind.\n",
        "\n",
        "Para todo lo otro, existen bibliotecas de código abierto, gratuitas y altamente adaptables.\n",
        "\n",
        "---\n",
        "\n",
        "**PyTorch** es una biblioteca de código abierto para aprendizaje automático y deep learning desarrollada por Meta (antes Facebook). Es ampliamente utilizada en investigación y producción debido a su diseño flexible y amigable para el desarrollo de modelos. Además, incluye la biblioteca torchVision, que tiene datasets predefinidos, transformaciones de datos y modelos preentrenados para visión por computadora. PyTorch se utiliza para tareas como **visión por computadora**, **procesamiento de lenguaje natural** (NLP), y **sistemas de recomendación**.\n",
        "\n",
        "---\n",
        "\n",
        "**Hugging Face** es una plataforma y biblioteca centrada en modelos de lenguaje natural (NLP) y deep learning. Su biblioteca más conocida, **Transformers**, proporciona acceso a miles de modelos preentrenados de última generación. Es el lugar al que acudir si quieres usar modelos avanzados como BERT, GPT, y T5. Adempas, incluye una colección enorme de datasets estandarizados para NLP y otros dominios. Hugging Face es ideal para tareas como **análisis de sentimientos**, **traducción automática**, y **generación de texto** con modelos como ChatGPT o BERT.\n"
      ],
      "metadata": {
        "id": "YLHlzNNcbxil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Pytorch\n",
        "\n",
        "\n",
        "\n",
        "Dataset **Caltech101**: Dataset con imágenes de objetos de 101 categorías diferentes, como flores, coches, guitarras, etc\n",
        "\n",
        "Modelo **ResNet**: un modelo preentrenado para mostrar cómo pasar imágenes por una red profunda y obtener salidas como categorías."
      ],
      "metadata": {
        "id": "Pz8w9buGeWIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos librerías necesarias\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "6-QviLFthY0u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir transformaciones para las imágenes\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Redimensionar a 224x224 para ResNet\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización para datos entre -1 y 1\n",
        "])\n",
        "\n",
        "# Descargar y cargar el dataset Caltech101\n",
        "train_data = datasets.Caltech101(root='./data', download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukNvcmHUir1P",
        "outputId": "9b46dd29-25a0-4708-e5ee-d54288ad074e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de algunas imágenes\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "# Mostrar las primeras 8 imágenes\n",
        "fig, axes = plt.subplots(1, 8, figsize=(16, 2))\n",
        "for i in range(8):\n",
        "    axes[i].imshow(images[i].permute(1, 2, 0))  # Cambiar dimensiones de canales para matplotlib\n",
        "    axes[i].set_title(f'Label: {labels[i].item()}')\n",
        "    axes[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jd0NMGUzit9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar un modelo preentrenado (ResNet18)\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Modificar la última capa para adaptarla a las clases de Caltech101 (101 clases)\n",
        "num_classes = len(train_data.categories) # 101\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Pasar las imágenes por el modelo\n",
        "with torch.no_grad():  # Desactivar gradientes (no entrenamos ahora)\n",
        "    outputs = model(images)  # Predicciones sin entrenar\n",
        "    probabilities = torch.nn.functional.softmax(outputs, dim=1)  # Convertir a probabilidades\n",
        "    predictions = probabilities.argmax(dim=1)  # Índices de las clases predichas"
      ],
      "metadata": {
        "id": "-DnQUFLOhXVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar predicciones y etiquetas reales\n",
        "print(\"\\nResultados:\")\n",
        "for i in range(8):\n",
        "    true_label = train_data.categories[labels[i].item()]  # Convertir etiqueta a nombre\n",
        "    predicted_label = train_data.categories[predictions[i].item()]  # Predicción\n",
        "    print(f\"Imagen {i+1}: Real -> {true_label}, Predicho -> {predicted_label}\")"
      ],
      "metadata": {
        "id": "dqhdI92njSfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar imágenes con predicciones superpuestas\n",
        "fig, axes = plt.subplots(1, 8, figsize=(16, 2))\n",
        "for i in range(8):\n",
        "    axes[i].imshow(images[i].permute(1, 2, 0))  # Cambiar dimensiones de canales para matplotlib\n",
        "    axes[i].set_title(f'{predicted_label}', color='green' if true_label == predicted_label else 'red')\n",
        "    axes[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tpAZo-2VjTc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.1 ¡Mejorando el modelo!\n",
        "\n",
        "¡OJO! Correr esto tomará tiempo"
      ],
      "metadata": {
        "id": "JXOxFOWEkAEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# Transformaciones para Caltech101\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convertir a 3 canales\n",
        "    transforms.Resize((224, 224)),  # Redimensionar a 224x224 para ResNet\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización para datos entre -1 y 1\n",
        "])\n",
        "\n",
        "# Cargar datos\n",
        "train_data = datasets.Caltech101(root='./data', download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "\n",
        "# Cargar modelo preentrenado ResNet50\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_classes = len(train_data.categories)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)  # Ajustar última capa\n",
        "model = model.to(\"cuda\")  # Usar GPU si está disponible\n",
        "\n",
        "# Configurar pérdida y optimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)  # Ajustar solo la última capa\n",
        "\n",
        "# Entrenar por algunas épocas\n",
        "model.train()\n",
        "for epoch in range(3):  # Usar más épocas para mejor rendimiento\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        print(f\"Entrenando épocs {epoch + 1}\")\n",
        "\n",
        "        images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Época {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Evaluar el modelo en un batch\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    data_iter = iter(train_loader)\n",
        "    images, labels = next(data_iter)\n",
        "    images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "    outputs = model(images)\n",
        "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "    predictions = probabilities.argmax(dim=1)\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"\\nResultados:\")\n",
        "for i in range(8):\n",
        "    true_label = train_data.categories[labels[i].item()]\n",
        "    predicted_label = train_data.categories[predictions[i].item()]\n",
        "    print(f\"Imagen {i+1}: Real -> {true_label}, Predicho -> {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKbmLXqIkHbi",
        "outputId": "f388f2cb-133b-4e24-e5c6-7ab036ffe2a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 1.297113208045416\n",
            "Epoch 2, Loss: 0.3376486148301731\n",
            "Epoch 3, Loss: 0.22279570078449873\n",
            "\n",
            "Resultados:\n",
            "Imagen 1: Real -> crab, Predicho -> crab\n",
            "Imagen 2: Real -> ewer, Predicho -> ewer\n",
            "Imagen 3: Real -> crayfish, Predicho -> crayfish\n",
            "Imagen 4: Real -> Faces, Predicho -> Faces\n",
            "Imagen 5: Real -> Motorbikes, Predicho -> Motorbikes\n",
            "Imagen 6: Real -> crayfish, Predicho -> crayfish\n",
            "Imagen 7: Real -> hawksbill, Predicho -> hawksbill\n",
            "Imagen 8: Real -> Motorbikes, Predicho -> Motorbikes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Huggingface\n",
        "\n",
        "**Dataset:** Datos de IMDb, que contienen reseñas a películas. Esto nos ayudará a clasificarlas.\n",
        "\n",
        "**Modelo**: Trabajaremos con un modelo preentrenado como BERT o DistilBERT de Hugging Face para procesar texto. Incluiremos la carga de un dataset, preprocesamiento y predicción."
      ],
      "metadata": {
        "id": "xAnnXVBYedUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar huggingface (si no lo tienes ya instalado)\n",
        "!pip install transformers datasets"
      ],
      "metadata": {
        "id": "CnmZSaxdeF32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "# Cargar un dataset de texto (ejemplo: IMDb para clasificación de reseñas)\n",
        "dataset = load_dataset(\"imdb\", split=\"test[:100]\")  # Usar solo 100 ejemplos para pruebas"
      ],
      "metadata": {
        "id": "h6rkV8pEkchK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar algunas reseñas\n",
        "print(\"\\nEjemplo de texto:\")\n",
        "print(dataset[0][\"text\"])\n",
        "print(\"\\nEtiqueta:\", dataset[0][\"label\"])  # Etiqueta: 0 = Negativo, 1 = Positivo"
      ],
      "metadata": {
        "id": "VhaxvGuUkex5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar un pipeline preentrenado para análisis de sentimientos\n",
        "classifier = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "j7fLCk-1mZsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pasar un ejemplo por el modelo\n",
        "result = classifier(dataset[0][\"text\"])\n",
        "print(\"\\nResultado del modelo para una reseña:\")\n",
        "print(f\"Texto: {dataset[0]['text'][:300]}...\")  # Mostrar los primeros 300 caracteres del texto\n",
        "print(f\"Predicción: {result[0]['label']}, Confianza: {result[0]['score']:.2f}\")\n",
        "\n",
        "# Visualizar resultados en múltiples reseñas\n",
        "print(\"\\nResultados detallados en las primeras 5 reseñas:\")\n",
        "for i in range(5):\n",
        "    text = dataset[i][\"text\"]\n",
        "    pred = classifier(text)\n",
        "    label = pred[0]['label']\n",
        "    confidence = pred[0]['score']\n",
        "\n",
        "    # Mostrar una porción del texto con la predicción\n",
        "    print(f\"\\nReseña {i+1}:\")\n",
        "    print(f\"Texto: {text[:300]}...\")  # Limitar a los primeros 300 caracteres para legibilidad\n",
        "    print(f\"Predicción: {label} (Confianza: {confidence:.2f})\")\n"
      ],
      "metadata": {
        "id": "EZHDkgPvOBZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1.1: Probemos nuestras propias reseñas"
      ],
      "metadata": {
        "id": "AK49BNbkoP0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"\\nEscribe una reseña para clasificar (o escribe 'salir' para terminar):\\n\")\n",
        "    if user_input.lower() == \"salir\":\n",
        "        print(\"¡Gracias por probar el modelo!\")\n",
        "        break\n",
        "\n",
        "    # Pasar la reseña escrita por el modelo\n",
        "    result = classifier(user_input)\n",
        "    label = result[0]['label']\n",
        "    confidence = result[0]['score']\n",
        "\n",
        "    # Mostrar el resultado de clasificación\n",
        "    print(f\"\\nResultado del modelo:\")\n",
        "    print(f\"Predicción: {label} (Confianza: {confidence:.2f})\")\n"
      ],
      "metadata": {
        "id": "NGBwkeY0oF5p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}