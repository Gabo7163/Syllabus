{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IIC2233 2024-2. Semana 16: Tópicos Avanzados 2 (Sistemas recomendadores)\n",
        "\n",
        "Demo práctica para recomendar anime usando 3 algoritmos distintos:\n",
        "- _Most Popular_\n",
        "- Basado en Contenidos con KNN.\n",
        "- Filtrado Colaborativo con KNN.\n",
        "- Híbrido (Colaborativo y Contenido) con Redes Neuronales\n",
        "\n",
        "Demo creada por el profesor [Hernán Valdivieso](https://hernan4444.github.io/).\n",
        "\n",
        "Los datos utilizados son de este [dataset](https://www.kaggle.com/datasets/hernan4444/anime-recommendation-database-2020) creado por el mismo profesor con información de [MyAnimeList](https://myanimelist.net/)."
      ],
      "metadata": {
        "id": "9rCU4aCxmH19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 1 - Datos"
      ],
      "metadata": {
        "id": "cl9ojvSoHElU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descargar datos"
      ],
      "metadata": {
        "id": "IDn1EvwsHG1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizamos la librería [`gdown`](https://pypi.org/project/gdown/) para descargar 2 datasets:\n",
        "1. El dataset de [anime](https://drive.google.com/file/d/1KhLqWalpy4YmcGbu4av1qx40yJoaKJxm/view?usp=drive_link).\n",
        "2. El dataset con las [interacciones usuario-anime-rating](https://drive.google.com/file/d/1C3h0bM11cxKxobQrks-Grgttgb7yvagK/view?usp=drive_link). Se ocupó el _dataset_ que solo tiene animes que fueron completamente visto a la fecha de obtener la información."
      ],
      "metadata": {
        "id": "ZxJs0tdKm9EQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1KhLqWalpy4YmcGbu4av1qx40yJoaKJxm\n",
        "!gdown 1C3h0bM11cxKxobQrks-Grgttgb7yvagK\n",
        "\n",
        "# Extra: modelo pre-entrenado para el última modelo que veamos\n",
        "# spoiler: no es tan bueno.\n",
        "!gdown 1P-iU7oYbmithNg0hSU2woTk8XkGmZk8v"
      ],
      "metadata": {
        "id": "JCJOQ_ZCACxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además, para el último algoritmo queremos recomendar en función de un usuario real (el profesor en este caso). Por lo cual, se recomienda:\n",
        "1. Tener cuenta en MyAnimeList\n",
        "2. [Visitar este enlace](https://myanimelist.net/panel.php?go=export)\n",
        "3. Descargar lista de animes\n",
        "4. Cambiar nombre de archivo por `animelist.xml.gz`\n",
        "5. Posicionar el archivo junto a este código.\n",
        "\n",
        "En caso de no tener cuenta, no hay que preocuparse. Podemos crear manualmente esto."
      ],
      "metadata": {
        "id": "QEoawzZ5nOAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar datos"
      ],
      "metadata": {
        "id": "AKf87g2oHCKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empezamos cargando ambos dataset (anime e interacciones) y observamos su contenido."
      ],
      "metadata": {
        "id": "Vk_NBB0dnvoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "df_anime = pd.read_csv('anime_actualizado.csv')\n",
        "df_rating = pd.read_csv('rating_complete.csv')\n",
        "\n",
        "print(df_anime.shape)\n",
        "df_anime.head()"
      ],
      "metadata": {
        "id": "D4DnbdEFAKEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_anime.Type.unique()"
      ],
      "metadata": {
        "id": "OUXqnL0noSxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_anime = df_anime[df_anime.Score != \"Unknown\"]\n",
        "\n",
        "df_anime.loc[:, \"Score\"] = df_anime.Score.astype(float)\n",
        "df_anime = df_anime[df_anime.Score > 5]\n",
        "\n",
        "tipos_aceptados = set([\"TV\", \"Movie\", \"OVA\"])\n",
        "df_anime = df_anime[df_anime.Type.isin(tipos_aceptados)]\n",
        "\n",
        "df_anime = df_anime.reset_index(drop=True)\n",
        "print(df_anime.shape)\n",
        "df_anime.head()"
      ],
      "metadata": {
        "id": "HmN9E3Fynq68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_rating.shape)\n",
        "df_rating.head()"
      ],
      "metadata": {
        "id": "QVGeXWerARbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solo de curioso, vamos a ver cuantas interacciones por calificación hay."
      ],
      "metadata": {
        "id": "zlsQLWVzynoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ax = df_rating.groupby(\"rating\").size().plot(kind=\"bar\")\n",
        "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-oczEAcPwyB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a reducir la cantidad para no considerar animes que no tenemos metadata (eliminará aprox 7 millones de datos)."
      ],
      "metadata": {
        "id": "8pBMxNiwrWES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Id_aceptados = set(df_anime.MAL_ID.unique())\n",
        "df_rating = df_rating[df_rating.anime_id.isin(Id_aceptados)]\n",
        "print(df_rating.shape)\n"
      ],
      "metadata": {
        "id": "IDTyYiNrnfm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigue siendo mucho, mejor nos quedamos con un random de 1 millon para usar despues."
      ],
      "metadata": {
        "id": "_k0-ceac0iWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rating = df_rating.sample(1000000).reset_index(drop=True)\n",
        "print(df_rating.shape)\n",
        "df_rating.head()"
      ],
      "metadata": {
        "id": "zGliwj3s0oEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifiquemos si la distribución de ratings sigue igual"
      ],
      "metadata": {
        "id": "IeVyr2GC0q3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ax = df_rating.groupby(\"rating\").size().plot(kind=\"bar\")\n",
        "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xphI-CkzrRQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener las frecuencias de las clases (ratings)\n",
        "class_counts = Counter(df_rating[\"rating\"])\n",
        "\n",
        "# Calcular pesos inversamente proporcionales a las frecuencias\n",
        "class_weights = {rating: 1.0 / count for rating, count in class_counts.items()}\n",
        "df_rating[\"weight\"] = df_rating[\"rating\"].map(class_weights)\n",
        "df_rating.head()"
      ],
      "metadata": {
        "id": "aM73P8i7jloK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos"
      ],
      "metadata": {
        "id": "QMTEaIlOHJOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a construir cada modelo de recomendación. Partiendo del más fácil en implementar al más complejo."
      ],
      "metadata": {
        "id": "n5fkrOosoaTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Most Popular"
      ],
      "metadata": {
        "id": "y75psvlLDyT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este algoritmo solo considera la cantidad de _views_ del anime. Así que solo debemos contar cuanta gente lo evaluó para saber cuál es el más visto. El _rating_ del anime nunca es considerado."
      ],
      "metadata": {
        "id": "dZzGrRPcooAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def most_popular_recommendations(df_rating, n_recommendations=10):\n",
        "    # Contar la cantidad de usuarios que registraron cada anime\n",
        "    anime_views_df = df_rating.groupby('anime_id')['user_id'].count().reset_index()\n",
        "    anime_views_df.rename(columns={'user_id': 'views_count'}, inplace=True)\n",
        "\n",
        "    # Ordenar los animes por la cantidad de registros (los más populares primero)\n",
        "    most_popular_animes = anime_views_df.sort_values('views_count', ascending=False)\n",
        "\n",
        "    # Obtener los n_recommendations animes más populares\n",
        "    recommendations = most_popular_animes.head(n_recommendations)\n",
        "\n",
        "    data_final = []\n",
        "    for i, row in recommendations.iterrows():\n",
        "        # Obtener la información\n",
        "        anime_info = df_anime[df_anime['MAL_ID'] == row.anime_id].iloc[0]\n",
        "        data = {\n",
        "            \"MAL_ID\": \"\",\n",
        "            \"Name\": \"\",\n",
        "            \"views\": row.views_count\n",
        "        }\n",
        "        data.update(anime_info.to_dict())\n",
        "        data_final.append(data)\n",
        "\n",
        "    df_final = pd.DataFrame(data_final)\n",
        "    return df_final\n",
        "\n",
        "# Ejemplo de uso\n",
        "recommendations = most_popular_recommendations(df_rating, 5)\n",
        "\n",
        "recommendations"
      ],
      "metadata": {
        "id": "wKWQuubKD0x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtrado basado en Contenido"
      ],
      "metadata": {
        "id": "mnW1alQbFz0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a ir con un algoritmo un poco más complejo. Para esto, se hará lo siguiente:\n",
        "1. Cada anime se convertirá en un _embedding_ que represente su información. Para esto vamos a pre-procesar todos los datos para que sean numéricos.\n",
        "2. Vamos a comprimir el _embedding_ para que sea de un tamaño más chico.\n",
        "3. Entrena 2 modelos (k-nearest neighbors - KNN) para determinar, dado 1 anime, cuáles son los más cercanos a él. Un modelo ocupará el _embedding_ original mientras otro ocupará la versión comprimida.\n",
        "4. Finalmente, dado 1 anime de la base de datos, vamos a recomendar 10 anime más.\n"
      ],
      "metadata": {
        "id": "ZrKIXE_EoxCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Librerías necesarias"
      ],
      "metadata": {
        "id": "09knc2tpe6e4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero hacemos _import_ de las librerías necesarias para esta parte:"
      ],
      "metadata": {
        "id": "xNdQGwgGpdl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, MinMaxScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "metadata": {
        "id": "aeTilNnlGz3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparar dataset"
      ],
      "metadata": {
        "id": "KukahSkZe8zA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos quedamos solo con las columnas que usaremos en la recomendación basada en contenido."
      ],
      "metadata": {
        "id": "L04pBV4ng8_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columnas_necesarias = [\"MAL_ID\", \"Name\", \"Score\", \"Genres\", \"Type\",\n",
        "                       \"Episodes\", \"Premiered\", \"Studios\", \"Source\", \"Rating\",\n",
        "                       \"Members\"]\n",
        "\n",
        "df_contenido = df_anime.loc[:, columnas_necesarias]\n",
        "df_contenido.head(5)"
      ],
      "metadata": {
        "id": "fx3lO1Wze_dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procesamos algunas columnas. En particular,\n",
        "* Generos y Estudio, como son atributos con muliples valores, los vamos a transformar en un lista.\n",
        "* Puntaje y Episodios, puede existir animes con valor `\"Unknown\"` que vamos a transformar en 0."
      ],
      "metadata": {
        "id": "OpQfg2UwhAlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_multilabel(series):\n",
        "    series = series.split(\", \")\n",
        "    if \"Unknown\" in series:\n",
        "        series.remove(\"Unknown\")\n",
        "    return series\n",
        "\n",
        "# Atributos con múltiples valores\n",
        "df_contenido[\"Genres\"] = df_contenido[\"Genres\"].map(process_multilabel)\n",
        "df_contenido[\"Studios\"] = df_contenido[\"Studios\"].map(process_multilabel)\n",
        "\n",
        "# Atributo con Unknown\n",
        "df_contenido[\"Episodes\"] = df_contenido[\"Episodes\"].replace(\"Unknown\", 0).astype(int)\n",
        "\n",
        "df_contenido.head()"
      ],
      "metadata": {
        "id": "2OOW3_v9YnSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, vamos a transformar todas las columnas con información a algo numérico. Para esto:\n",
        "1. Usaremos `one-hot-encoding` para todas las columnas categóricas. Es decir, si tengo `[\"A\", \"B\"]`, se crearán 2 columnas: `\"Columna_A\"` y `\"Columna_B\"` donde habrá un 1 si el anime tiene ese valor o un 0 si no lo tiene.\n",
        "2. Todas las columnas numéricas serán escaladas para que estén en un rango de 0-1."
      ],
      "metadata": {
        "id": "uP9i3rimhlQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para procesar categorías\n",
        "def preprocessing_category(df, column, is_multilabel=False):\n",
        "    lb = LabelBinarizer()\n",
        "    if is_multilabel:\n",
        "        lb = MultiLabelBinarizer()\n",
        "\n",
        "    expandedLabelData = lb.fit_transform(df[column])\n",
        "    labelClasses = lb.classes_\n",
        "\n",
        "    # Create a pandas.DataFrame from our output\n",
        "    category_df = pd.DataFrame(expandedLabelData, columns=labelClasses)\n",
        "    del df[column]\n",
        "    return pd.concat([df, category_df], axis=1)\n",
        "\n",
        "anime_metadata = df_contenido.copy()\n",
        "\n",
        "# Procesar categorías\n",
        "anime_metadata = preprocessing_category(anime_metadata, \"Type\")\n",
        "anime_metadata = preprocessing_category(anime_metadata, \"Premiered\")\n",
        "anime_metadata = preprocessing_category(anime_metadata, \"Studios\", is_multilabel=True)\n",
        "anime_metadata = preprocessing_category(anime_metadata, \"Genres\", is_multilabel=True)\n",
        "anime_metadata = preprocessing_category(anime_metadata, \"Source\")\n",
        "anime_metadata = preprocessing_category(anime_metadata, \"Rating\")\n",
        "\n",
        "# Quedarme con las\n",
        "ID_NAME = anime_metadata[[\"MAL_ID\", \"Name\"]]\n",
        "\n",
        "del anime_metadata[\"MAL_ID\"]\n",
        "del anime_metadata[\"Name\"]\n",
        "del anime_metadata[\"Unknown\"]\n",
        "\n",
        "scaled_data = MinMaxScaler().fit_transform(anime_metadata[[\"Score\", \"Episodes\", \"Members\"]])\n",
        "anime_metadata[[\"Score\", \"Episodes\", \"Members\"]] = scaled_data\n",
        "\n",
        "anime_metadata_values = anime_metadata.values\n",
        "\n",
        "print(anime_metadata.shape)\n",
        "anime_metadata.head()"
      ],
      "metadata": {
        "id": "R33YMlcrYnVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear un nuevo dataset donde vamos a \"comprimir\" las 910 columnas en solo 200 columnas"
      ],
      "metadata": {
        "id": "4pDQaT1jkkze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svd = TruncatedSVD(n_components=200)\n",
        "anime_svd_values = svd.fit_transform(anime_metadata_values)\n",
        "\n",
        "# Crear un nuevo DataFrame con las coordenadas\n",
        "anime_svd_df = pd.DataFrame(data=anime_svd_values)\n",
        "\n",
        "# Concatenar con el dataframe original para tener la información del nombre\n",
        "anime_svd_df = pd.concat([ID_NAME, anime_svd_df], axis=1)\n",
        "anime_svd_df.head()"
      ],
      "metadata": {
        "id": "PYhvfmt3YnYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenar modelo"
      ],
      "metadata": {
        "id": "37rss0x9fUXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos 2 modelos, uno que ocupe la información original y otro que ocupe la comprimida"
      ],
      "metadata": {
        "id": "dUJdO_SXkqkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear modelo KNN con distancia coseno para encontrar vectores cercanos.\n",
        "model_knn_metadata = NearestNeighbors(metric='cosine', n_neighbors=10)\n",
        "model_knn_svd = NearestNeighbors(metric='cosine', n_neighbors=10)\n",
        "\n",
        "model_knn_metadata.fit(csr_matrix(anime_metadata_values))\n",
        "model_knn_svd.fit(csr_matrix(anime_svd_values))"
      ],
      "metadata": {
        "id": "aAvMrZDcYnbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recomendar"
      ],
      "metadata": {
        "id": "RyS6friVfXl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear función que ocupa el modelo para entregar recomendación dado 1 anime\n",
        "def get_recommended(modelo, anime_vector, vecinos=10):\n",
        "    distances, indices = modelo.kneighbors(anime_vector, n_neighbors=vecinos)\n",
        "    indices, distances = indices.flatten(), distances.flatten()\n",
        "    result = []\n",
        "\n",
        "    # Partimos en 1 porque la posición 0 siempre será el mismo anime qe le solicitamos\n",
        "    for i in range(1, len(distances.flatten())):\n",
        "        data = {\"MAL_ID\": \"\", \"Name\": \"\", \"distancia\": distances[i]}\n",
        "        anime_recomendado = df_contenido.iloc[indices[i]].to_dict()\n",
        "        data.update(anime_recomendado)\n",
        "        result.append(data)\n",
        "\n",
        "    return pd.DataFrame(result)"
      ],
      "metadata": {
        "id": "g08NZO9SfZgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escogemos 1 anime según su ID en MyAnimelist"
      ],
      "metadata": {
        "id": "zn1WJkWHlJN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kimi no Na wa --> MAL_ID == 32281\n",
        "indice = ID_NAME[ID_NAME.MAL_ID == 32281].index[0]\n",
        "\n",
        "df_contenido.iloc[[indice]]"
      ],
      "metadata": {
        "id": "eJ92q_U8fh5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recomendamos usando nuestro modelo que ocupa metadata"
      ],
      "metadata": {
        "id": "VSMXMgIelIv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anime_vector = anime_metadata_values[indice,:].reshape(1, -1)\n",
        "get_recommended(model_knn_metadata, anime_vector, 10)"
      ],
      "metadata": {
        "id": "MK8Wgp_VgbL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recomendamos usando nuestro modelo que ocupa la información comprimida"
      ],
      "metadata": {
        "id": "muJRdEA2lQKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anime_vector = anime_svd_values[indice,:].reshape(1, -1)\n",
        "get_recommended(model_knn_svd, anime_vector, 10)"
      ],
      "metadata": {
        "id": "wDfwR5xCk8O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtrado Colaborativo"
      ],
      "metadata": {
        "id": "cye6IX2JrN2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a realizar el mismo tipo de recomendación, pero usando otro vector de entrada:\n",
        "\n",
        "1. Cada anime se convertirá en un _embedding_ que represente su información. Para esto se ocuparán las interacciones.\n",
        "3. Entrenar 1 modelos (k-nearest neighbors - KNN) para determinar, dado 1 anime, cuáles son los más cercanos a él.\n",
        "4. Finalmente, dado 1 anime de la base de datos, vamos a recomendar 10 anime más.\n"
      ],
      "metadata": {
        "id": "7DWwIQoLLpuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Librerías Necesarias"
      ],
      "metadata": {
        "id": "bmJay0EHrmzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "B7ftD5v7rX7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rating_2 = pd.read_csv('rating_complete.csv')\n",
        "Id_aceptados = set(df_anime.MAL_ID.unique())\n",
        "df_rating_2 = df_rating_2[df_rating_2.anime_id.isin(Id_aceptados)]\n",
        "print(df_rating_2.shape)"
      ],
      "metadata": {
        "id": "jTBE4rh4rRbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_user = df_rating_2['user_id'].value_counts()\n",
        "count_user = set(count_user[count_user > 300].index)\n",
        "\n",
        "count_anime = df_rating_2['anime_id'].value_counts()\n",
        "count_anime = set(count_anime[count_anime > 500].index)"
      ],
      "metadata": {
        "id": "_EbzFQgRrTR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U6MM6ejx3FnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rating_data = df_rating_2[df_rating_2['user_id'].isin(count_user)]\n",
        "rating_data = rating_data[rating_data['anime_id'].isin(count_anime)].reset_index(drop=True)\n",
        "\n",
        "del count_user\n",
        "del count_anime\n",
        "\n",
        "cantidad_usuario = len(rating_data.user_id.unique())\n",
        "cantidad_anime = len(rating_data.anime_id.unique())\n",
        "\n",
        "print(f\"Cantidad de usuarios: {cantidad_usuario}\")\n",
        "print(f\"Cantidad de animes: {cantidad_anime}\")\n",
        "print(f\"Cantidad interacciones: {len(rating_data)}\")\n",
        "print(f\"Tamaño matriz: {cantidad_usuario * cantidad_anime}\")"
      ],
      "metadata": {
        "id": "oB7DQN6xrURc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating_matrix = rating_data.pivot(index='anime_id', columns='user_id', values='rating')\n",
        "rating_matrix.fillna(0, inplace=True)\n",
        "\n",
        "print(rating_matrix.shape)\n",
        "rating_matrix.head()"
      ],
      "metadata": {
        "id": "7cGnbcwT1NVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento"
      ],
      "metadata": {
        "id": "_597cW5ArflN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "piviot_table_matrix = csr_matrix(rating_matrix)\n",
        "piviot_table_matrix"
      ],
      "metadata": {
        "id": "1E8QUUrurVL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\")\n",
        "model.fit(piviot_table_matrix)"
      ],
      "metadata": {
        "id": "yOEUFjBQuQBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recomendación"
      ],
      "metadata": {
        "id": "NOSBd-HmrqsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAL = 32281\n",
        "MAL = 16498\n",
        "MAL = 1575\n",
        "df_anime[df_anime.MAL_ID == MAL]"
      ],
      "metadata": {
        "id": "b43c93JIrsW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = rating_matrix.loc[MAL].values\n",
        "vector = vector.reshape(1, -1)\n",
        "\n",
        "distance, suggestions = model.kneighbors(vector, n_neighbors=6)\n",
        "distance = distance.flatten()\n",
        "suggestions = suggestions.flatten()\n",
        "\n",
        "MAL_ID = []\n",
        "for i in range(0, len(distance)):\n",
        "    matrix_index = suggestions[i]\n",
        "    anime_index = rating_matrix.index[matrix_index]\n",
        "    print('{0}: {1}, with distance of {2}:'.format(i, anime_index, distance[i]))\n",
        "    MAL_ID.append(df_anime[df_anime.MAL_ID == anime_index].iloc[0])\n",
        "\n",
        "pd.DataFrame(MAL_ID)"
      ],
      "metadata": {
        "id": "RtfosfN2rtd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminar variables para liberar ram"
      ],
      "metadata": {
        "id": "cyV9Gd2ttaNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del rating_matrix\n",
        "del df_rating_2\n",
        "del rating_data\n",
        "del model\n",
        "del piviot_table_matrix"
      ],
      "metadata": {
        "id": "KgqAMC3gtcQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo híbrido"
      ],
      "metadata": {
        "id": "5TRWwptFGxgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a utilizar el modelo más complejo, uno con redes neuronales. El enfoque de este modelo será predecir el _rating_ que un usuario le dará a un anime. De este modo, la recomendación se basará en dar los animes que se espera que tengan mayor _rating_."
      ],
      "metadata": {
        "id": "dx4N_14Epj2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Librerías necesarias"
      ],
      "metadata": {
        "id": "9X7_LvwNRS03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero hacemos _import_ de las librerías necesarias para esta parte:"
      ],
      "metadata": {
        "id": "fG_4XO3ERf6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "import numpy as np\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "aiSXNK9aRWU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocesamiento de los datos"
      ],
      "metadata": {
        "id": "fao4KFNYrqjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para procesar categorías\n",
        "def preprocessing_category(df, column, is_multilabel=False):\n",
        "    lb = LabelBinarizer()\n",
        "    if is_multilabel:\n",
        "        lb = MultiLabelBinarizer()\n",
        "\n",
        "    expandedLabelData = lb.fit_transform(df[column])\n",
        "    labelClasses = lb.classes_\n",
        "\n",
        "    # Create a pandas.DataFrame from our output\n",
        "    category_df = pd.DataFrame(expandedLabelData, columns=labelClasses)\n",
        "    del df[column]\n",
        "    return pd.concat([df, category_df], axis=1)\n",
        "\n",
        "anime_metadata = df_contenido.copy()\n",
        "\n",
        "# Procesar categorías\n",
        "anime_metadata = preprocessing_category(anime_metadata, \"Type\")\n",
        "anime_metadata = preprocessing_category(anime_metadata, \"Premiered\")\n",
        "anime_metadata = preprocessing_category(anime_metadata, \"Studios\", is_multilabel=True)\n",
        "anime_metadata = preprocessing_category(anime_metadata, \"Genres\", is_multilabel=True)\n",
        "anime_metadata = preprocessing_category(anime_metadata, \"Source\")\n",
        "anime_metadata = preprocessing_category(anime_metadata, \"Rating\")\n",
        "\n",
        "del anime_metadata[\"MAL_ID\"]\n",
        "del anime_metadata[\"Name\"]\n",
        "del anime_metadata[\"Unknown\"]"
      ],
      "metadata": {
        "id": "q7832GZH2glk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Anime\")\n",
        "display(df_anime.head(2))\n",
        "\n",
        "print(\"\\nRatings\")\n",
        "display(df_rating.head(2))\n",
        "\n",
        "print(\"\\nMetadata procesada\")\n",
        "display(anime_metadata.head(2))"
      ],
      "metadata": {
        "id": "Wkx3KtDNsbYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COLUMNAS_METADATA = 910\n",
        "\n",
        "# Dado el ID de un anime, obtengo su vector de metadata\n",
        "item_metadata = {}\n",
        "values_metadata = anime_metadata.values.tolist()\n",
        "anime_id = df_anime[\"MAL_ID\"].values.tolist()\n",
        "for i, id_ in enumerate(anime_id):\n",
        "    item_metadata[id_] = values_metadata[i]\n",
        "\n",
        "del values_metadata\n",
        "del anime_id\n",
        "\n",
        "# Dado el ID de un usuario, obtengo la lista de animes que vio\n",
        "animes_per_user = df_rating[[\"user_id\", \"anime_id\"]].groupby(\"user_id\")['anime_id'].apply(list)\n",
        "\n",
        "print(item_metadata[1][:9])\n",
        "print(item_metadata[5][:9])\n",
        "\n",
        "usuario = list(animes_per_user.keys())[0]\n",
        "df_anime.loc[df_anime[\"MAL_ID\"].isin(animes_per_user[usuario])]"
      ],
      "metadata": {
        "id": "Hxpgnf6319hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confección del modelo\n"
      ],
      "metadata": {
        "id": "yjRCxpte3leE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        torch.nn.init.constant_(m.bias, 0.01)\n",
        "\n",
        "class MyAnimeListRecomendador(nn.Module):\n",
        "    def __init__(self, metadata_dim, embedding_dim, hidden_dim, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa para obtener los embeddings de los ítems a partir de sus metadatos\n",
        "        self.item_metadata_fc = nn.Linear(metadata_dim, embedding_dim)\n",
        "\n",
        "        # Capa para obtener los embeddings de los ítems a partir de sus metadatos\n",
        "        self.user_metadata_fc = nn.Linear(metadata_dim, embedding_dim)\n",
        "\n",
        "        # Capas ocultas para modelar la interacción usuario-ítem\n",
        "        self.hidden_layer1 = nn.Linear(embedding_dim*2, hidden_dim)\n",
        "        self.hidden_layer2 = nn.Linear(hidden_dim, hidden_dim*2)\n",
        "\n",
        "        # Función de activación y regularización\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
        "\n",
        "        self.apply(initialize_weights)\n",
        "\n",
        "    def forward(self, user_metadata, item_metadata):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        - user_item_metadata: Tensor de metadatos de los ítems que el usuario ha consumido (batch_size, user_embedding_dim)\n",
        "        - item_metadata: Tensor de metadatos del ítem a predecir (batch_size, metadata_dim)\n",
        "        \"\"\"\n",
        "        # Obtener embedding para el ítem actual\n",
        "        item_embedding = self.item_metadata_fc(item_metadata)\n",
        "\n",
        "        # Obtener embedding para el usuario actual\n",
        "        user_embedding = self.user_metadata_fc(user_metadata)\n",
        "\n",
        "        # Concatenar los embeddings del usuario y del ítem\n",
        "        interaction = torch.cat([user_embedding, item_embedding], dim=-1)\n",
        "\n",
        "        # Pasar la concatenación por las capas ocultas\n",
        "        x = self.hidden_layer1(interaction)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.hidden_layer2(x)\n",
        "\n",
        "        # Predicción final\n",
        "        output = self.fc(x).squeeze()\n",
        "        return output"
      ],
      "metadata": {
        "id": "vvTwzRYV3o7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "VHJeWbE4rugC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para sumar cada metadata de los videos visto por un usaurio\n",
        "def obtener_embedding_usuario(item_metadata, animes_visto_usuario):\n",
        "    user_metadata = np.zeros(COLUMNAS_METADATA)\n",
        "    for anime_id in animes_visto_usuario:\n",
        "        user_metadata += np.array(item_metadata[anime_id])\n",
        "\n",
        "    return user_metadata\n",
        "\n",
        "# Dataset personalizado para el entrenamiento, para cada tripleta de\n",
        "# (usuario, anime, rating) genera los datos necesarios para el modelo.\n",
        "class InteractionDataset(Dataset):\n",
        "    def __init__(self, interactions, item_metadata, animes_per_user):\n",
        "        self.interactions = interactions\n",
        "        self.item_metadata = item_metadata\n",
        "        self.anime_per_user = animes_per_user\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.interactions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user_id, item_id, rating, _ = self.interactions.iloc[idx]\n",
        "        animes_visto_usuario = self.anime_per_user[user_id]\n",
        "\n",
        "        user_metadata = obtener_embedding_usuario(self.item_metadata,\n",
        "                                                  animes_visto_usuario)\n",
        "\n",
        "        user_metadata -= np.array(self.item_metadata[item_id])\n",
        "        if len(animes_visto_usuario) > 1:\n",
        "            user_metadata = user_metadata/(len(animes_visto_usuario) - 1)\n",
        "\n",
        "        return [\n",
        "            torch.tensor(user_metadata, dtype=torch.float32),\n",
        "            torch.tensor(self.item_metadata[item_id], dtype=torch.float32),\n",
        "            torch.tensor(rating, dtype=torch.float32)\n",
        "        ]"
      ],
      "metadata": {
        "id": "avf_vCyTOCqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entrenar(modelo, batch_size, epochs, learning_rate, max_samples, save):\n",
        "    # Dataset de entrenamiento\n",
        "    dataset = InteractionDataset(df_rating, item_metadata, animes_per_user)\n",
        "\n",
        "    # Elementos para el entrenamiento\n",
        "    criterion = nn.MSELoss()  # Usamos MSE para la predicción de ratings\n",
        "    optimizer = optim.Adam(modelo.parameters(), lr=learning_rate)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Empezamos entrenamiento\n",
        "    modelo.train()\n",
        "    modelo = modelo.to(device)\n",
        "\n",
        "    # Crear un sampler que tome muestras balanceadas\n",
        "    sampler = WeightedRandomSampler(df_rating[\"weight\"], num_samples=max_samples, replacement=False)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        print(f\"Época {epoch + 1}\")\n",
        "        for user_embedding, item_metadata_embedding, ratings in tqdm(dataloader):\n",
        "            user_embedding = user_embedding.to(device)\n",
        "            item_metadata_embedding = item_metadata_embedding.to(device)\n",
        "            ratings = ratings.to(device)\n",
        "\n",
        "            # Obtener predicciones\n",
        "            preds = modelo(user_embedding, item_metadata_embedding)\n",
        "            #print(user_embedding.sum(), item_metadata_embedding.sum(), ratings)\n",
        "\n",
        "            # Calcular error de la predicción\n",
        "            loss = criterion(preds, ratings)\n",
        "\n",
        "            # Ajustar parámetros del modelo en función del error\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        print()\n",
        "        print(f\"\\tLoss Promedio: {total_loss / len(dataloader)}\")\n",
        "        if save:\n",
        "            torch.save(modelo.state_dict(), 'MyAnimeListRecomendador.pth')"
      ],
      "metadata": {
        "id": "Lm4z01SSsXuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento del modelo, versión clases"
      ],
      "metadata": {
        "id": "KDlF0MScuytw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constantes\n",
        "batch_size = 10\n",
        "epochs = 3\n",
        "learning_rate = 0.01\n",
        "max_samples = 500\n",
        "\n",
        "modelo = MyAnimeListRecomendador(COLUMNAS_METADATA, embedding_dim=512, hidden_dim=512)\n",
        "entrenar(modelo, batch_size, epochs, learning_rate, max_samples, save=False)"
      ],
      "metadata": {
        "id": "w107nTELuYq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento del modelo, versión extendida (para dejarlo en casa corriendo).\n",
        "\n",
        "**Importante** Según las contantes ocupadas, esto puede consumir muchas hroas."
      ],
      "metadata": {
        "id": "a6msVBIsuZz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "epochs = 1\n",
        "learning_rate = 0.01\n",
        "max_samples = 20\n",
        "\n",
        "modelo = MyAnimeListRecomendador(COLUMNAS_METADATA, embedding_dim=512,\n",
        "                                 hidden_dim=512)\n",
        "\n",
        "if os.path.exists('MyAnimeListRecomendador.pth'):\n",
        "    print(\"Cargando pesos\")\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    pesos = torch.load('MyAnimeListRecomendador.pth',\n",
        "                       map_location=device, weights_only=True)\n",
        "    modelo.load_state_dict(pesos)\n",
        "\n",
        "entrenar(modelo, batch_size, epochs, learning_rate, max_samples, save=False)"
      ],
      "metadata": {
        "id": "_1cAqEL9Qscl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recomendación"
      ],
      "metadata": {
        "id": "9n5aLiywrvg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí vamos a usar 2 modelos. El que entrenamos recientemente y otro que fue entrenado previamente por mucho más tiempo.\n",
        "\n",
        "Para ambos, vamos a probar con:\n",
        "\n",
        "* El usuario cargado al inicio de la demo (cuenta MyAnimeList).\n",
        "* Un usuario totalmente nuevo (no ha visto anime).\n",
        "* Un usuario creado ahora con algunos animes que diga el público."
      ],
      "metadata": {
        "id": "v89EYpqPrxWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COLUMNAS_METADATA = 910\n",
        "\n",
        "modelo_preentrenado = MyAnimeListRecomendador(COLUMNAS_METADATA,\n",
        "                                embedding_dim=512,\n",
        "                                hidden_dim=512)\n",
        "\n",
        "# Cargamos los pesos (en caso de existir)\n",
        "if os.path.exists('MyAnimeListRecomendador.pth'):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    pesos = torch.load('MyAnimeListRecomendador.pth', map_location=device, weights_only=True)\n",
        "    modelo_preentrenado.load_state_dict(pesos)"
      ],
      "metadata": {
        "id": "vW9GcIplTFw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def obtener_embedding_usuario_v2(item_metadata, animes_visto_usuario):\n",
        "    user_metadata = np.zeros(COLUMNAS_METADATA)\n",
        "    count = 0\n",
        "    for anime_id in animes_visto_usuario:\n",
        "        if anime_id in item_metadata:\n",
        "            user_metadata += np.array(item_metadata[anime_id])\n",
        "            count += 1\n",
        "    return user_metadata, count\n",
        "\n",
        "def recommend_animes(model, item_metadata, user_animes, df_anime, top_n=10):\n",
        "    # Create user embedding\n",
        "    user_embedding, count = obtener_embedding_usuario_v2(item_metadata, user_animes)\n",
        "    if count > 0:\n",
        "        user_embedding /= count\n",
        "\n",
        "    user_embedding = torch.tensor(user_embedding, dtype=torch.float32)\n",
        "\n",
        "    # Get all anime IDs not in the user's list\n",
        "    all_anime_ids = set(df_anime[\"MAL_ID\"])\n",
        "    user_anime_ids = set(user_animes)\n",
        "    anime_ids_to_predict = list(all_anime_ids - user_anime_ids)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    user_embedding = user_embedding.to(device)\n",
        "\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for anime_id in anime_ids_to_predict:\n",
        "            item_metadata_embedding = torch.tensor(item_metadata[anime_id], dtype=torch.float32)\n",
        "            item_metadata_embedding = item_metadata_embedding.to(device)\n",
        "\n",
        "            prediction = model(user_embedding, item_metadata_embedding).item()\n",
        "            predictions.append((anime_id, prediction))\n",
        "\n",
        "    # Sort by predicted rating and get top N recommendations\n",
        "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_recommendations = predictions[:top_n]\n",
        "    bottom_recommendations = predictions[-5:]\n",
        "\n",
        "    animes_ids, ratings = zip(*(top_recommendations + bottom_recommendations))\n",
        "\n",
        "    recommended_animes = df_anime[df_anime[\"MAL_ID\"].isin(animes_ids)]\n",
        "    recommended_animes.loc[:, \"Rating\"] = ratings\n",
        "    columnas_deseadas = [\"MAL_ID\", \"Name\", \"Genres\", \"Score\", \"Type\", \"Rating\", \"Premiered\"]\n",
        "\n",
        "    return recommended_animes.loc[:, columnas_deseadas]"
      ],
      "metadata": {
        "id": "dRGRUflCSkzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_animes_recomendar = df_anime.copy()\n",
        "\n",
        "# Limitar posibles animes a recomendar\n",
        "df_animes_recomendar = df_animes_recomendar[df_animes_recomendar.Score > 8]\n",
        "df_animes_recomendar = df_animes_recomendar[df_animes_recomendar.Type.isin([\"TV\"])]\n",
        "df_animes_recomendar = df_animes_recomendar[~df_animes_recomendar['Genres'].str.contains('Unknown', na=False)]\n",
        "df_animes_recomendar = df_animes_recomendar[~df_animes_recomendar['Genres'].str.contains('Hentai', na=False)]\n",
        "df_animes_recomendar = df_animes_recomendar.reset_index(drop=True)\n",
        "\n",
        "print(df_animes_recomendar.shape)\n",
        "df_animes_recomendar.loc[:, [\"MAL_ID\", \"Name\", \"Genres\", \"Score\", \"Type\", \"Premiered\"]].head(10)"
      ],
      "metadata": {
        "id": "5uYjeyNinPBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Caso 1 - Usuario de MyAnimelist"
      ],
      "metadata": {
        "id": "uwGhkTQjUaFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Abrir el archivo anime.xml.gz con pandas\n",
        "df_anime_user = pd.read_xml('animelist.xml.gz',xpath=\".//anime\")\n",
        "user_anime_list = df_anime_user.loc[:, \"series_animedb_id\"].tolist()\n",
        "\n",
        "recommend_animes(modelo, item_metadata, user_anime_list, df_animes_recomendar)"
      ],
      "metadata": {
        "id": "OQgKzoT3UYu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_animes(modelo_preentrenado, item_metadata, user_anime_list, df_animes_recomendar)"
      ],
      "metadata": {
        "id": "5iQbg5hlmFkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Caso 2 - Usuario 100% nuevo"
      ],
      "metadata": {
        "id": "ocNkepyYUf7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_animes(modelo, item_metadata, [], df_animes_recomendar)"
      ],
      "metadata": {
        "id": "WjAYwIeTUhTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_animes(modelo_preentrenado, item_metadata, [], df_animes_recomendar)"
      ],
      "metadata": {
        "id": "srhoyisQmvwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Caso 3 - Usuario creado ahora"
      ],
      "metadata": {
        "id": "y90YE_HqUhl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_animes_recomendar.loc[:, [\"MAL_ID\", \"Name\", \"Genres\", \"Score\"]].sort_values(by=\"Score\", ascending=False).head(10)"
      ],
      "metadata": {
        "id": "2N_dYXJmm8vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series_animedb_id = [52991, 5114]\n",
        "recommend_animes(modelo, item_metadata, series_animedb_id, df_animes_recomendar)"
      ],
      "metadata": {
        "id": "DBxdQGKIl7rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_animes(modelo_preentrenado, item_metadata, series_animedb_id, df_animes_recomendar)"
      ],
      "metadata": {
        "id": "bi6TiJOdokWU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}